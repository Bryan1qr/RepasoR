---
title: "Análisis Estadístico Básico"
Subtitle: 'P.C.E. Data Science: Estadística y Análisis de Datos en R'
Author: "Bryan Quispe"
output: bookdown::epub_book
editor_options:
  chunk_output_type: console
---

# **1. Motivación para la clase**

Trabajaremos emulando la información del artículo científico <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7875321/>

## Emulemos los resultados del estudio

```{r eval=FALSE}
# Creemos los conjuntos de datos adelantándonos un poco con el trabajo de funciones en R: rnorm()
set.seed(1000)
todos <- rnorm(100,mean=47.1,sd=14.8)
todos

mean(todos) #47.09958
sd(todos) #14.73796

set.seed(1000)
ns <- rnorm(25,59.1,11.5)
mean(ns) # 56.57751

set.seed(1000)
so <- rnorm(75,43.0,13.6)
mean(so) # 42.002
# Veamos los histogramas de frecuencias de datos en las tres variables

par(mfrow=c(1,3))
hist(todos)
hist(ns)
hist(so)
dev.off()
```

Comparemos los grupos creados en conjunto y los grupos creados por separado.

```{r eval=FALSE}
# Unamos los vectores de los grupos que fueron creados por separado
ambos <- c(ns,so)

par(mfrow=c(1,2))
hist(todos)
hist(ambos)
dev.off()
mean(ambos) #45.64588
# Obtengamos el promedio y la desviación estándar para comparar con el objeto que contenía la información de los 100 individuos creado inicialmente
sd(ambos) #14.11177

```

La estadística descriptiva nos permite dar a conocer las características de un conjunto de datos. Más adelante aplicaremos pruebas estadísticas a este caso de estudio.

# **2. Variable Aleatoria**

Una variable aleatoria es un conjunto de resultados (outcomes) de un proceso aleatorio conocido como experimento estadístico. Esta serie de números cuantifica el experimento.

```{r eval=FALSE}
# Creemos una variable 
set.seed(1000)
poblacion <- rnorm(1000, 47.1, 14.8)

m1 <- sample(x=poblacion,size = 30)
m2 <- sample(poblacion,30)
m3 <- sample(poblacion,30)

data.frame(m1,m2,m3) %>% colMeans()
#  m1       m2       m3 
# 48.66364 46.16377 47.97548 
```

Veamos la distribución de la frecuencia de los valores de la población de 1000 individuos y la población inicial de 100 individuos.

```{r eval=FALSE}
# Creemos un histograma simple comparando ambas bases de datos
par(mfrow=c(2,1))
hist(todos)
hist(ambos)
dev.off()
```

# **3. Muestreo aleatorio en R**

Todo en investigación se basa en el simple hecho de realizar una muestra. La aleatorización de estos procesos son de suma importancia para garantizar la reproducibilidad del estudio, ya que los resultados no estarán sujetas a subjetividades, por el contrario, mostrarán la realidad factible del conjunto de datos que comprende la población de estudio. Realicemos muestreos sencillos en R a partir de vectores.

```{r eval=FALSE}
# Función sample()
sample(todos,3)

# Muestreo sin reemplazamiento
sample(todos,25,replace = FALSE)
sample(todos,75,replace = FALSE)

# Muestreo con reemplazamiento
sample(todos,300, replace = TRUE)

# Muestreo aleatorio reproducible en R usando antes la función set.seed()
set.seed(1000)
sample(todos,25)
```

# **4. Independencia de las variables**

Este es un concepto que **no puede ser testeado** por pruebas estadísticas ni alguna función dentro de R.

*Dos muestras son independientes si:*

-   El resultado de uno no afecta al otro.

-   Esto es: al tomar una muestra (observación) esta no afecta al valor de la siguiente muestra.

*Dos muestras son dependientes si:*

-   Mientras que si al tomar una muestra, esta acción afecta a la toma de las siguientes

Debemos de pensar cómo hemos tomado los datos para estar seguros si nuestro muestreo es independiente o no. **Esto afectará la toma de decisiones** más adelante cuando realicemos análisis estadísticos.

```{r eval=FALSE}
# Muestreos sin reemplazamiento son ejemplo de muestras dependientes la probabilidad de que un número del 1 al 40 sea muestreado cambia cuando realizamos una muestra
edad <- c(rep("adulto",3),rep("niño",2))

# Hagamos muestreos sin reemplazamiento
set.seed(10)
sample(edad,4,replace = FALSE)
```

Debido al muestro sin reemplazamiento, estas son muestras dependientes, la toma de una muestra afecta la probabilidad de las que aún no han sido muestreadas.

**¿Cuál es la probabilidad de en el quinto muestreo obtengamos "niño"?**

Rpta/. la probabilidad de obtener "niño" en el siguiente muestreo es 1

Si realizamos un muestreo con reemplazamiento, las muestras son independientes, por tanto:

```{r eval=FALSE}
# Hagamos muestreos con reemplazamiento
set.seed(10)
sample(edad,4,replace = TRUE)
```

**¿Cuál es la probabilidad de en el quinto muestreo obtengamos "niño"?**

Debemos recurrir a la teoría de probabilidades discretas para responder esta pregunta:

Rpta/. la probabilidad de obtener "niño" en el siguiente muestreo es 2/5 = 0.4

Instalar librerías nuevas

```{r eval=FALSE}
install.packages("latticeExtra")
```

Librerías a usar

```{r eval=FALSE}
library(tidyverse)
library(psych)
library(latticeExtra)
```

# **5. Probabilidades Discretas**

Las probabilidades discretas describen la probabilidad de ocurrencia de cada valor de una variable aleatoria discreta.

En conjuntos de datos discretos, la probabilidad de ocurrencia de un evento es

$$
Pr(A)= \text{probabilidad del evento A}  = \text{proporción del evento A}
$$

Lo que se puede calcular con la **fórmula**:

$$
Pr(A)= \frac{\text{# resultados favorables al evento A posibles}}{\text{# total de resultados posibles}}
$$

```{r eval=FALSE}
# Probabilidades en un dado
dado <- 1:6
sample(dado,1,replace = TRUE)
```

Veamos la importancia del `n` muestral para definir probabilidades de eventos discretos utilizando la:

## Simulación de Monte Carlo

```{r eval=FALSE}
# Usa el dado virtual creado en el chunk anterior, y
# genera una muestra de 10,000 lanzamientos aleatorios
resultados <- sample(dado,1000,replace = T)

# Obtener la proporción de cada resultado de los lanzamientos
tabal <- table(sample(dado,1000,replace = T))
prop.table(tabal)
```

Adicionalmente, como ya hemos ido viendo en el primer curso de Introducción a R para Ciencias, `mean()`, que es la función para calcular promedios, también se usa para contar valores verdaderos `TRUE` de un conjunto de datos lógicos.

```{r eval=FALSE}
# Importante uso de mean()
# resultados==6
mean(resultados==6) 
```

***Regresemos a las diapositivas para recordar la importancia de la independencia.***

## **5.1. Probabilidades discretas para muestras Dependientes**

Recordemos el ejemplo de la probabilidade extrar la etiqueta `"niño"` o `"adulto"` de la variable edad.

```{r eval=FALSE}
edad <- c(rep("adulto",3),rep("niño",2))
```

¿Cómo afecta la independencia al cálculo de la probabilidad?. Para calcular probabilidades consecutivas cuando las muestras son dependientes, hablamos de ***probabilidades condicionales***. Primero, veamos la probabilidad del evento 1 `Pr(A)`.

$$
Pr(A)=Pr(\text{probabilidad del evento 1 = `Niño`})= 2/5
$$

Luego de obtener esa primera muestra, calculamos la probabilidad de que el segundo evento sea niño, dado que el primero fue niño `Pr(B|A)`.

$$
Pr(B∣A)=Pr(\text{probabilidad del evento 2 = `Niño`}∣\text{probabilidad del evento 1 = `Niño`})=1/4
$$

¿Cuál es la probabilidad de obtener Niño y Niño consecutivamente `Pr(A y B)`?.

$$
Pr(\text{A y B}) = Pr(A) * Pr(B∣A) = 2/5 * 1/4 = 2/20 = 0.1
$$

## **5.2. Probabilidades discretas para muestras Independientes**

En casos de muestras independientes, no hay condicionales. Si el ejemplo anterior hubiera sido *con reemplazamiento*, entonces las muestras son independientes, y por tanto las probabilidades se multiplican de esta manera. ¿Cuál es la probabilidad de obtener Niño dos veces consecutivas?

$$
Pr(\text{A y B})=Pr(A)×Pr(B)=2/5*2/5=0.4*0.4=0.16
$$

# **6. Probabilidades Continuas**

Las probabilidades continuas describen la probabilidad de los posibles valores de una variable aleatoria continua. Una variable aleatoria continua es una variable aleatoria con un conjunto de valores posibles (conocido como rango o intervalo) que es infinito e incontable. Esto quiere decir, para distribuciones continuas, la probabilidad de un solo valor no está definida. Estas distribuciones de probabilidades se pueden representar con la **función de distribución acumulativa** **empírica** (**ECDF**).

## **6.1. Función de distribución acumulativa empírica** **ECDF**

```{r eval=FALSE}
# Carga la base de datos "alturas.xlsx"
alturas <- openxlsx::read.xlsx("alturas.xlsx")

# Grafíca un eCDF para comprender mejor las notaciones matemáticas
library(latticeExtra)
#dos formas de plotear ecdf
ecdfplot(alturas$cm)
ecdfplot(~cm,data = alturas)
```

La **eCDF** es una función de distribución para datos, llamada `F(a)`, continuos que informa la ***proporción*** de los datos debajo de un valor especificado `a`:

$$
F(a)=Pr(x≤a) = \text{proporción de valores menores igual a `a`}
$$

Esta proporción nos aproxima a la ***probabilidad*** `Pr(x≤a)` de que un valor `x` se encuentre dentro de un rango del mínimo del conjunto de datos hasta `a`:

$$
F(a)=Pr(x≤a) = \text{probabilidad de que x ∈ al rango min(X) hasta `a`}
$$

Si quiero hallar la probabilidad `Pr(x>a)` de que un valor `x` sea superior al rango desde el mínimo del conjunto de datos hasta `a`, debo calcular el complemento `1-F(a)`:

$$
Pr(x>a)=1−F(a)=1-Pr(x≤a)
$$

La probabilidad de que una observación esté entre dos valores `a` y `b`, por tanto será:

$$
F(b)−F(a)
$$

### Ejemplo ECDF

Se tiene un conjunto de mediciones de altura de varias personas de género masculino y femenino. Se desea conocer la ***probabilidad*** de que al escoger de manera aleatoria una persona de género masculino, este tenga más de 180 cm de altura, *dado que todas las observaciones tienen la misma probabilidad de ser escogidas.* Para calcular esto debemos calcular la **función de distribución acumulativa empírica** **ECDF** `F(a)`.

$$
Pr(x>180)=1−Pr(x≤180)=1−F(180)
$$

```{r eval=FALSE}
# Trabajaremos con la variable generada al cargar el excel alturas.xlsx
# Realiza un subset seleccionando unicamente valores para el sexo Masculino
# pull extrae un vector de una columna
masc <- filter(alturas,sexo=="Masculino") %>% pull(cm)
head(masc)
# Crea una función llamada CDF para calcular el CDF de un conjunto de datos
mean(masc <= 180) #0.6539409

ECDF <- function(x,y){
  mean(x <= y)
}
# Calcular la probabilidad de que al tomar al azar un elemento
# de la variable Masc, este tenga talla inferior a 180
ECDF(masc,180) # 0.6539409

# Calcular la probabilidad de que al tomar al azar un elemento
# de la variable Masc, este tenga talla superior a 180
1-ECDF(masc,180) #0.3460591

# Gráfico eCDF
ecdfplot(masc) # porque es vector, sale más facil que un df
```

#### **--- Ejercicio 01**

Carga la base de datos `storms`, hallada dentro de la librería `dplyr`. Calcula la probabilidad de que un huracán (filtra los valores `hurricane` en la columna `status`) tenga un diámetro de tormenta (columna `ts_diameter`) superior a 600 km, menor igual a 500 km, y entre 200 y 400 km.

```{r eval=FALSE}
# Activar base de datos precargada en R 
data("storms")

# Filtra la base para cumplir lo solicitado
#usamos un truco para tratar con nas
HURRI <- filter(storms,
                status=="hurricane" &
                  !is.na(ts_diameter)) %>%
  pull(ts_diameter)



# probabilidad de ts_diameter superior a 600 km
1-ECDF(HURRI,600) #0.03330069

# probabilidad de ts_diameter menor igual a 500 km
ECDF(HURRI,500) #0.9392752

# probabilidad de ts_diameter entre 200 y 400 km
ECDF(HURRI,400)-ECDF(HURRI,200) # 0.5925563

```

## **6.2 Función de distribución acumulativa CDF**

Es posible calcular una aproximación a estos resultados si conocemos a qué distribución teórica de probabilidades corresponde nuestra variable de estudio. En este caso, nos adelantamos y decimos que las mediciones de altura corresponden a una **distribución de probabilidades Normal.**

La distribución Normal se usa, por tanto, como una aproximación muy útil de muchos eventos naturales. Podemos obtener la CDF (no la empírica sino la teórica) de la distribución Normal en R la función `pnorm()`, la cual contiene la formula matemática para su cálculo.

En nuestro ejemplo, solo necesitamos los valores que describen mi población, no los valores de muestreo. Estos son, la altura *promedio* y la *desviación estándar* para calcular la probabilidad de un intervalo.

```{r eval=FALSE}
# Dado el promedio y desviacion estándar
prom <- mean(masc)
de <- sd(masc)
# ¿Cuál es la aproximación normal de la probabilidad de que una persona 
# mida HASTA 180 cm, dado el promedio y la desviación estándar proporcionada?
pnorm(180,prom,de) # 0.6662659
ECDF(masc,180) # 0.6539409

# ¿Cuál es la probabilidad de que una persona mida MÁS DE 180 cm dado
# el promedio y la desviación estándar proporcionada?
1-pnorm(180,prom,de) # 0.3337341

# Veamos la probabilidad de que un hombre mida entre 150 y 170 cm
ECDF(masc,170)-ECDF(masc,150) #0.1871921

# Y su aproximación normal
pnorm(170,prom,de)-pnorm(150,prom,de) # 0.2521713
```

#### **--- Ejercicio 02**

Calcula las aproximaciones normales de los valores hallados en el ejercicio 1.

```{r eval=FALSE}
med <- mean(HURRI)
des <- sd(HURRI)
# probabilidad de ts_diameter superior a 600 km
1-ECDF(HURRI,600) # 0.03330069
1-pnorm(600,med,des) # 0.01089032

# probabilidad de ts_diameter menor igual a 500 km
ECDF(HURRI,500) # 0.9392752
pnorm(500,mean(HURRI),sd(HURRI)) # 0.940454

# probabilidad de ts_diameter entre 200 y 400 km
ECDF(HURRI,400)-ECDF(HURRI,200) # 0.5925563
pnorm(400,mean(HURRI),sd(HURRI))-pnorm(200,mean(HURRI),sd(HURRI))
# 0.5363016
```

## **6.3 Función de Densidad de Probabilidad**

Hemos definido previamente que para las distribuciones continuas la probabilidad de un valor específico no puede ser definido, solamente la probabilidad de pertenecer a un rango. Sin embargo, el concepto de **función de densidad de probabilidad** (le llamaremos *fun.dens*) nos permite obtener, bajo una definición teórica: la **densidad de probabilidad** `f(x)`, un valor calculado que nos aproxime a dicha probabilidad puntual. Esta densidad es definida como:

$$
F(a)=Pr(x≤a)=\int_∞^af(x)dx
$$

```{r eval=FALSE}
# La función que calcula la función de densidad de probabilidad normal es:
dnorm()
```

Creemos la *función de densidad de probabilidad* para el conjunto de datos de tallas para el género Masculino `Masc`.

```{r eval=FALSE}
# Veamos los valores mínimos y máximos
min(masc)
max(masc)
range(masc)


# Creemos una nueva data frame df que contenga los datos originales
# y su respectivos valores de probabilidades usando la aproximación
# normal con la función dnorm()
numeros <- seq(min(masc),max(masc),length=100)
funs.dens <- dnorm(numeros,mean(masc),sd(masc))
df <- data.frame(numeros,funs.dens)

# Visualiza la df
View(df)

# Grafíca de la curva dfs
plot(df, type="l", col="deepskyblue1")

# Así es como luce la fds Normal (modo ggplot2)
ejemplo <- seq(-4, 4, length = 1000)
data.frame(ejemplo, f = dnorm(ejemplo)) %>%
  ggplot(aes(ejemplo, f)) +
  geom_line(color="deepskyblue1")+
  geom_area(fill="cadetblue1", alpha=0.5)+
  xlab("Datos Normales")+
  ylab("Función de Densidad")+
  theme_bw()

df %>% ggplot(aes(x=numeros,y=funs.dens))+
  geom_line(color="deepskyblue1")+
  geom_area(fill="cadetblue1",alpha=0.5)
```

### **`rnorm()`:** Números Aleatorios Normales y la Simulación de Monte Carlo

En R existen funciones que permiten obtener números aleatorios (random) en función de una distribución teórica de probabilidades. La función `rnomr()` genera numeros aleatorios para la distribución normal. Sus argumentos son: número de elementos a crear, promedio y desviación estándar del conjunto:

```{r eval=FALSE}
rnorm(100, mean=0, sd=1)
```

Veamos un ejemplo de su gran utilidad:

```{r eval=FALSE}
# ¿Cuál es la talla más alta dentro de Masc?
max(masc)
```

La pregunta es: ¿Siempre será así?. Si re-muestreo mi población ¿puede ser que encuentre valores máximos más altos o más bajos?, ¿cuáles serán tales probabilidades?

```{r eval=FALSE}
# Para resolver esto, creemos una función 
# para realizar, en base a un subset definido,
# la simulación de MonteCarlo n veces, y obtener 
# el valor de talla más alto en cada simulación 

altosMC <- function(subset, n){
   replicate(n, {
  datos.simulados <- rnorm(1000, mean(subset), sd(subset))
  max(datos.simulados)
  })
}

# Utilicemos el subset Masc para simular los valor valores
# de talla más altos 10000 veces. Asignemos el conjunto
# de datos resultante al nombre personas.altas

personas.altas <- altosMC(masc,10000)

# Veamos la probabilidad de que las personas más altas
# tengan tallas superiores a 200, 210, 220, 225 cm
mean(personas.altas>200)
1-ECDF(personas.altas,200)
1-ECDF(personas.altas,210)
1-ECDF(personas.altas,220)
1-ECDF(personas.altas,220)

```

#### **--- Ejercicio 03**

Aplica lo aprendido hasta el momento. Crea un conjunto de datos aleatorios, llamado `data.e3`, que tenga promedio `20` y desviación estándar `2`. Visualiza el valor mínimo y máximo. Luego, calcula la probabilidad de que un valor tomado aleatoriamente del conjunto sea menor igual a `20`. Finalmente, crea el eCDF con la función `ecdfplot()` del paquete `latticeExtra`.

```{r eval=FALSE}
# crear un conjunto de datos normales de 300 elementos, 
# con promedio 20 y desviación estándar 2 
set.seed(500)
data.e3 <- rnorm(300,mean = 20,sd=2)

# Veamos los valores mínimos y máximos
range(data.e3)

# Calcula la probabilidad de que un valor aleatorio sea menor igual a 20
mean(data.e3<=20) # 0.5366667
ECDF(data.e3,20) # 0.5366667
# Grafíca el ECDF con la función de latticeExtra::ecdfplot()
ecdfplot(data.e3)
DF <- data.e3
# Grafíca la función de densidad del conjunto de datos
range(DF)
numeros2 <- seq(min(DF), max(DF), length = 100)
fun.dens2 <- dnorm(numeros2, mean(DF), sd(DF))
df2 <- data.frame(numeros2, fun.dens2)
plot(df2, type="l", col="deepskyblue1")

#histograma
par(mfrow=c(2,1))
hist(data.e3)
plot(df2, type="l", col="deepskyblue1")
dev.off()
```

**¿Un ECDF sería homólogo a un histograma de frecuencias?**

Rpta/. sí.

# **7. Funciones para Probabilidades en R**

Aquí tienes una lista de funciones asociadas con estos tres puntos que hemos cubierto.

-   Las funciones que comienzan con `p` como `pnorm()` generan datos de la **"función de distribución acumulativa" (CDF)**.

-   Las que comienzan con `d` como `dnorm()` generan datos de la **"función de densidad de probabilidad" (fun.dens)**.

-   Las que comienzan con `r` como `rnorm()` generan **números aleatorios (random)** según la distribución de probabilidades específica:

    -   `norm` para distribución Normal,

    -   `pois` distribución de Poisson,

    -   `binom` distribución Binomial,

    -   `beta` distribución Beta,

    -   `t` distribución T Student,

    -   `gamma` distribución Gamma,

    -   `chisq` distribución Chi-Cuadrado,

    -   `exp` distribución Exponencial.

```{r eval=FALSE}
# Funciones para generar "función de distribución acumulativa" (CDF)
pnorm()
ppois()
pbinom()
pbeta()
pt()
pgamma()
pchisq()
pexp()

# Funciones para generar "función de densidad de probabilidad" (fun.dens)
dnorm()
dpois()
dbinom()
dbeta()
dt()
dgamma()
dchisq()
dexp()

# Funciones para generar números aleatorios siguiento una distribución
rpois()
rbinom()
rbeta()
rt()
rgamma()
rchisq()
rexp()
```

# 8. Medidas de Tendencia Central

El **análisis exploratorio de datos** debe comenzar por el cálculo o graficación del *eCDF*, pero es muy usual saltarse esto e ir directamente al cálculo de los estadísticos: promedio, mediana, moda. Estos tres son conocidos como medidas de tendencia central.

Usemos las funciones para el cálculo de las medidas de tendencia central:

-   `mean()`

-   `psych::geometric.mean()`

-   `psych::harmonic.mean()`

-   `median()`

-   `moda()`

Para revisar las funciones a usar, creemos los siguientes conjuntos de datos

```{r eval=FALSE}
# Creemos los conjuntos de datos siguientes
a <- c(1,2,3,1,2,1,1,1,2,3,2,2,3,2)
b <- c(1,3,1,2,1,1,1,2,3,2,2,3,250)
c <- c(1,2,3,1,2,1,1,1,2,3,2,2,3,NA)
d <- c(TRUE,TRUE,TRUE,FALSE,FALSE)
e <- c("a","b","a","a","b","a")
peso <- c(63.96, 71.38, 75.42, 58.27, 72.14)
altura <- c(1.60,1.69,1.88,1.59, 1.90)
IMC <- peso / (altura**2)  # Equivalente a elevar al cuadrado con ^2
```

Calculemos las medidas de tendencia central

```{r eval=FALSE}
# Media aritmética --------------------------
# Promedio de conjuntos de datos sin NAs
mean(a)
mean(b)
# Promedio de conjuntos de datos con NAs
mean(c,na.rm = T)

# El otro uso de mean(): contar valores TRUE
# y brindar la proporción de verdaderos
mean(c(T,T,T,T,F,F))

# Media geométrica --------------------------
library(psych)
# Comparemos el resultado de mean() con geometric.mean()
geometric.mean(b)
mean(b)

# media harmónica
harmonic.mean(IMC)
# Mediana --------------------------
median(b)

# Moda --------------------------
# Funcion para datos con mas de una moda
# O para datos no numericos
moda <- function(data) {
  unicos <- unique(data)
  conteo <- tabulate(match(data, unicos))
  unicos[conteo == max(conteo)]
}

# Calcular la moda de a,c,e
moda(e)
```

#### **--- Ejercicio 04**

Carga la base de datos `Puromycin` de la librería `datasets`. La base contiene información sobre la **velocidad de reacción una enzima** del grupo de estudio que fue tratado (columna `state`valor `treated`). El objetivo es obtener el promedio para cada concentración de Puromicina usada (columna `conc`). Decide qué promedio usarás y realiza el manejo de la base de datos usando `dplyr`.

```{r eval=FALSE}
data("Puromycin")
view(Puromycin)
str(Puromycin)

Puromycin %>% 
  group_by(conc) %>% 
  summarise(mediah=harmonic.mean(rate))

Puromycin %>% 
  group_by(state) %>% 
  summarise(mediah=harmonic.mean(rate))

Puromycin %>% 
  group_by(state,conc) %>% 
  summarise(mediah=harmonic.mean(rate))
```

# 9. Medidas de Dispersión

Un segundo grupo de mediciones que se deben realizar en el **análisis exploratorio de datos** son aquellas que describen cómo están dispersos los datos.

Usemos las funciones para el cálculo de las medidas de dispersión:

-   `min()`

-   `max()`

-   `range()`

-   `quantile()`

-   `sd()`

-   `var()`

-   `summary()`

Para revisar las funciones a usar, creemos los siguientes conjuntos de datos

```{r eval=FALSE}
# Creemos los conjuntos de datos siguientes
f <- rnorm(20, mean=0,sd=1)
g <- rpois(20, lambda = 2) 
```

Calculemos las medidas de tendencia central

```{r eval=FALSE}
# Minimo y Maximo --------------------------
range(f) # -1.952219  1.441893
range(g) # 0 5
# Rango --------------------------

# Desviación estándar y Varianza --------------------------
sd(f)
sd(g)

var(f) #sd al cuadrado
var(g)
# Calcula el cuantile
data("iris")
summary(iris)
# par mostrar los cuantiles específicos de una variable:
quantile(iris$Sepal.Length)
quantile(iris$Petal.Length)
# para obtener los estimado calculados como SPSS
quantile(iris$Petal.Length, type = 6)
# Plot de cuantiles
hist(iris$Petal.Length)
boxplot(iris$Petal.Length)
```

#### **--- Ejercicio 05**

Carga la base de datos `msleep` de la librería `tidyverse`. La base contiene información sobre cuánto duermen los animales mamíferos. Calcula el promedio `geometric.mean()` y la varianza `var()` de la cantidad de horas de sueño que tienen (columna `sleep_total`), horas despierto (columna `awake`) y el peso corporal (columna `bodywt`), tomando en cuenta su dieta como factor de agrupamiento (columna `vore`). Antes de iniciar, asegúrate primero de que en la columna `vore` no existan valores `NA`. Recuerda que `geometric.mean()` pertenece a la librería `psych`.

```{r eval=FALSE}
data("msleep")
colSums(is.na(msleep))
# forma 1
msleep %>% filter(!is.na(vore)) %>% 
  group_by(vore) %>% 
  select(a=awake,b=bodywt,st=sleep_total) %>% 
  summarise_all(.funs = c(media=mean,var=var))

# forma 2 # la mejor xd
msleep %>% filter(!is.na(vore)) %>% 
  group_by(vore) %>% 
  select(a=awake,b=bodywt, st=sleep_total) %>% 
  summarise_all(.funs = c(geo=geometric.mean,var=var))
# forma3
msleep %>% filter(!is.na(vore)) %>% 
  group_by(vore) %>% 
  select(awake,bodywt, sleep_total) %>% 
  summarise_all(list("mgeo"=harmonic.mean,
                     varianza=var))

#forma4
msleep %>% filter(!is.na(vore)) %>% 
  group_by(vore) %>% 
  summarise(hm_bw=harmonic.mean(awake),
            hm_aw=harmonic.mean(awake))
```

# 10. Conceptos Estadísticos Relevantes

En esta sección nos basaremos en entender cómo el z-score/t-score son importantes en para identificar el margen de error de la muestra, y con ello los intervalos `CI` de confianza del promedio hallado. Lo que el `CI` nos indica es que, con una certeza del 95% asumimos que el promedio real de la población de donde se obtuvieron los datos se encuentra dentro de esas cifras, siendo μ el promedio reportado para la muestra.

## Convertir un conjunto de datos a Z-score

```{r eval=FALSE}
# Extra la columna sleep_total de la base de datos msleep
sue <- msleep$sleep_total

# Crea un histograma con ella
hist(sue)
# Generemos el plot de la función de densidad de estos datos
plot(density(sue))
abline(v=mean(sue))
view(sue)
# Generemos los Z-score
sue_escalado <- scale(sue)
abline(v=mean(sue_escalado))

# Generemos el plot de la función de densidad de estos datos escalados
plot(density(sue_escalado))
```

## arreglar alguna vez xd

```{r eval=FALSE}
nums <- seq(min(sue_escalado),max(sue_escalado),length=83)
ddd <- dnorm(nums,mean = 0,sd=1)
den_sc <- scale(ddd)
dfff <- data.frame(sue_escalado,ddd)


dfff %>% ggplot(aes(x=sue_escalado,y=ddd))+
  geom_area()
```

## Intervalo de Confianza y Margen de Error

```{r eval=FALSE}
# Creemos el mismo conjunto de datos que usamos en el ejercicio 3
set.seed(500)
dataset <- rnorm(300, mean=20, sd=4)

# Mide la longitud de dataset
n <- length(dataset)

# Calcula el promedio
promedio <- mean(dataset)

# Calcula el error estándar
# como la división entre la desviación estándar
# sobre la raíz cuadrada de n
error.estandar  <- sd(dataset)/sqrt(n)

# Calcula el valor crítico t-score
# correspondiente a la distribución T Student
# para el 97.5% de confianza (solo se una una cola),
# con grados de libertad n-1
t.critico <- qt(0.975,df=(n-1)) #1.96793

# Calcula el Margen de error de la muestra
# como t critico por el error estándar
margen.de.error <- t.critico * error.estandar

# Calcula los intervalos de confianza
(promedio - margen.de.error) 
(promedio + margen.de.error) 

# Verifica lo hallado con las funciones CI() y ci()
library(tidyverse)
Rmisc::CI(dataset)
gmodels::ci(dataset)
```

Instalar librerías nuevas

```{r eval=FALSE}
install.packages("moments")
install.packages("bestNormalize")
install.packages("dslabs")
install.packages("performance")
install.packages("qqplotr")
#install.packages("nortest")
install.packages("rstatix")
#install.packages("broom")
install.packages("DescTools")
install.packages("ggpubr")
install.packages("see")
install.packages("ggpubr")
install.packages("sjPlot")
```

Librerías a usar

```{r eval=FALSE}
library(tidyverse)
library(moments)
library(nortest)
library(rstatix)
library(bestNormalize)
library(dslabs)
library(broom)
library(ggpubr)
library(performance)
```

# **1. Pruebas de Normalidad**

Las técnicas que revisaremos a continuación serán utilizadas para determinar la normalidad de un conjunto de datos. Recuerda que dicho conjunto de datos puede ser:

-   Tus datos originales resultantes de un muestreo, o

-   Los residuales de un modelo estadístico sobre los cuales debe verificarse la normalidad.

```{r eval=FALSE}
library(kableExtra)

head(ToothGrowth) %>% kable(booktabs=TRUE) %>% 
  kable_styling(font_size = 12)
```

```{r eval=FALSE}
# Carga la base de datos ToothGrowth
data("ToothGrowth")
view(ToothGrowth)
# Cambiemos los valores de ToothGrowth
# para entender mejor la información
ToothGrowth$supp <- factor(ToothGrowth$supp,
                           labels = c("OJ"="JN",
                                      "VC"="AA"))

# Gráfico de densidad sencillo para toda la data
plot(density(ToothGrowth$len))

# Separar los grupos para luego poder verificar 
# la normalidad de cada uno de ellos por separado
JN <- ToothGrowth %>% filter(supp=="JN") %>% pull(len)
AA <- ToothGrowth %>% filter(supp=="AA") %>% pull(len)

# Gráficos de densidad de cada grupo
par(mfrow=c(1,2))
plot(density(JN),main = "Jugo de Naranja")
plot(density(AA),main = "Ácido Ascórbico")
dev.off()
```

## **1.1 Simetria y Curtosis**

Recordemos los rangos de valores para la simetría y curtosis.

**Simetría:**

-   *Asimetría positiva*: mayor a 0.5

-   *Simetría (normalidad)*: entre -0.5 y 0.5

-   *Asimetría negativa*: menor a -0.5

**Curtosis:**

-   *Leptocúrtica*: K > 3

-   *Mesocúrtica (normalidad)*: K \~ 3

-   *Platicúrtica*: K \< 3

```{r eval=FALSE}
# La funciones que usaremos están almacenadas en la librería moments
library(moments)

# Calculemos la Simetría de la base de datos ToothGrowth
skewness(ToothGrowth$len) # -0.1461768

# Calculemos la Curtosis de la base de datos ToothGrowth
# de la librería moments
kurtosis(ToothGrowth$len) # 2.024403

# aún así podría haber normalidad, no es muy lejano a 3
# Calcular la simetría y curstosis del grupo Jugo de Naranja y ácido 
# ascórbico
skewness(AA) # 0.2900364 es simétrica
skewness(JN) # -0.5504998 asimetria negativa 
kurtosis(AA) # 2.217982 Platicúrtica
kurtosis(JN) # 2.107601 platicúrtica

ToothGrowth %>% group_by(supp) %>% 
  select(len,supp) %>% summarise_if(is.numeric,lst(skewness))

ToothGrowth %>% group_by(supp) %>% 
  select(len,supp) %>% summarise_if(is.numeric,lst(kurtosis))

ToothGrowth %>% group_by(supp) %>% 
  select(len,supp) %>% summarise_if(is.numeric,lst(kurtosis))
```

## **1.2 Tests de Normalidad**

El objetivo es comprobar si la distribución empírica de un conjunto de datos (`eCDF`) encaja dentro de la distribución teórica Normal.

> **IMPORTANTE:** Deseamos observar p-valores MAYORES a 0.05, de tal manera que aceptemos la hipótesis nula "no existen diferencias significativas entre la distribución empírica de probabilidades de mis datos y la distribución teórica de probabilidades Normal".

-   H0: No existe diferencia entre la distribución teórica normal y la distribución empírica de los datos.

-   Ha: Existe diferencia entre la distribución teórica normal y la distribución empírica de los datos.

**Prueba de Kolmogorov-Smirnov (KS Test)**

```{r eval=FALSE}
# Test de Kolmogorov-Smirnov de JN y AA
ks.test(JN,"pnorm",mean(JN),sd(JN)) # p-value = 0.6152 
xd <- ks.test(AA,"pnorm",mean(AA),sd(AA)) # p-value = 0.9845
# > xd$p.value [1] 0.9844541
```

**Prueba de Shapiro-Wilk (SW Test)**

```{r eval=FALSE}
# Test de Shapiro-Wilk de JN y AA
# shapiro.test no es pipe friendly
shapiro_test(JN)
shapiro_test(AA)

# Test de Shapiro usando funciones Pipe-friendly y dplyr. 
# La función que usaremos está almacenada en la librería rstatix
# Trabajemos con la base de datos iris para calcular el test de SW
# agrupando por Species  
# Aplicar sobre shapiro_test() sobre varias columnas a la vez
library(rstatix)
iris %>% group_by(Species) %>% 
  shapiro_test(Petal.Length,Sepal.Length,Sepal.Width)
```

**Prueba de Anderson-Darling (AD Test)**

```{r eval=FALSE}
# La función que usaremos está almacenada en la librería nortest
library(nortest)

# Grafiquemos la densidad de la variable Ozone base de datos airquality 
plot(density(airquality$Ozone,na.rm = T),
     main = "Gráfico de densidad de Ozono",
     ylab = "Densidad",
     xlab = "Valores de concentración (ug/m3",
     col="purple")

# en el caso de cambiar nombres podemos hacer uso de fix sobre un vec
nomqual <- names(airquality)
fix(nomqual)
# Revisemos el simetría para corroborar
# lo que vemos en la gráfica
skewness(airquality$Ozone,na.rm = T) # 1.225681 asimetria posit

# Veamos el valor del AD Test
ad.test(AA) # p-value = 0.6669 normalidad
ad.test(JN) # p-value = 0.01647
xde <- ad.test(airquality$Ozone) # p-value = 2.787e-11 no normal

# AD Test para varios grupos con dplyr
# ambos métodos funcionan
ad.test.p <- function(x){
  prueba <- ad.test(x)
  prueba$p.value
}

# según el profe
ad.test.p <- function(x){
  test <- nortest::ad.test(x)
  return(as.vector(test$p.value))
}

# ahora el estadístico de ad
ad.test.stat <- ad.test.p <- function(x){
  prueba <- ad.test(x)
  prueba$statistic
}

# según el profe
ad.test.stat <- function(x){
  test <- nortest::ad.test(x)
  return(as.vector(test$statistic))
}

  
as.test.p(airquality$Ozone)
iris %>% 
  group_by(Species) %>% 
  summarise_at(.vars = 1:4,.funs = (ad.test.p))

airquality %>% group_by(Month) %>% 
  summarise_at(.vars = nomqual,.funs = ad.test.p)

# varias ambos estadísticos
airquality %>% 
  group_by(Month) %>% 
  summarise_at(.vars = nomqual,
               .funs = list(ad.test.stat,ad.test.p))

iris %>% 
  group_by(Species) %>% 
  summarise_at(.vars = 1:4,
               .funs = list(pvalor=ad.test.p,stat=ad.test.stat)) %>% 
  t() %>% View()
```

## **1.3 Q-Q Plot**

El gráfico cuantil-cuantil enfrenta los cuantiles del muestreo con sus cuantiles tóricos según la distribución Normal. Si los cuantiles del muestreo son idénticos o muy cercanos a los cuantiles teóricos, entonces los puntos se agregan en la línea diagonal del gráfico, la línea Q-Q con intercepto 0 y pendiente 1.

```{r eval=FALSE}
# Calculemos el Q-Q Plot Manualmente para entender su origen
# Usemos la base de datos de Ácido Ascórbico AA
dist.muestra <- sort(AA)
probabilidades <- ppoints(AA)
dist.teorica <-qnorm(probabilidades, mean(AA),sd(AA))

# REVISAR "?par de graphics"
plot(dist.teorica, dist.muestra,
     title("Gráfica Q-Q Plot"),
     xlab = "Distribución teórica",
     ylab = "Distribución muestral")
abline(a=0,b=1,col="purple",lwd=2)

# Ahora automaticemos el proceso con  qqnorm() y qqline()
# se usa vectores
qqnorm(AA)
qqline(AA, col="blue",lwd=2.1)

# SI HAY NORMALIDAD GRÁFICA
# comprobamos numéricamente:
shapiro_test(AA) # p=0.428

# Crea el Q-Q Plot de la base de datos Jugo de Naranja JN
qqnorm(JN)
qqline(JN,col="green",lwd=2.2)
# no hay normalidad gráfica
# ni normalidad numérica
shapiro_test(JN) # 0.0236
```

![Comparativa de QQplots](images/rstudio_gaGIEp6RVU.png "QQplots")

En la figura 1, en la primera gráfica, se aprecia valores de dispersión en los extremos que se alejan de la línea QQ esto se debe a que no se han quitado los valores atípicos, mas no implican ausencia de normalidad. En cuanto a la segunda se ve que almedio hay valores seguidos que no cumplen la semejanza, por lo que no hay normalidad.

# **2. Normalizar Datasets**

En algunas ocasiones necesitaremos realizar una transformación del conjunto de datos de tal manera que este se normalice, es decir, luzca más parecido a la distribución teórica normal de que originalmente es. Esto implica, tener simetría y curtosis normales, u obtener p-valores no significativos en los test de normalidad. Mayor información sobre el paquete bestNormalize en [Using the bestNormalize Package (r-project.org)](https://cran.r-project.org/web/packages/bestNormalize/vignettes/bestNormalize.html).

```{r eval=FALSE}
# La función que usaremos está almacenada en la librería bestNormalize
library(bestNormalize)

# Ejecutemos la función bestNormalize() con nuestros datos JN
# desactivamos los argumentos para tener una desición libre
bestNormalize(JN, out_of_sample = F, allow_orderNorm = F)

# Apliquemos la función de normalización indicada por bestNormalize()
Nueva_JN <- bestNormalize::boxcox(JN)
Nueva_JN$x.t # datos transformados

# Verificar con el gráfico Q-Q Plot
# vemos que no fue suficiente la transforamción para normalizar
qqnorm(Nueva_JN$x.t)
qqline(Nueva_JN$x.t)
# como último recurso se puede usar la de ordernorm

# Verificar con el test de Shapiro-Wilk
# si hay normalidad numérica  
shapiro.test(Nueva_JN$x.t) #p-value = 0.1008
```

Debemos tener cuidado al aplicar transformaciones en un solo grupo de un conjunto de datos. Es mejor transformar toda la base de datos (columna). Verifiquemos cómo afecta la transformación.

```{r eval=FALSE}
# Grafiquemos los boxplots AA, JN y Nueva_JN
boxplot(AA,JN, Nueva_JN$x.t, names = c("AA","JN","Nueva JN"))
# vemos la importancia de transformar todos los grupos de datos
```

Apliquemos la transformación a toda la columna `len` de la base de datos `ToothGrowth`. Luego, guardemos los datos transformados como una columna llamada `Nlen` dentro de la misma base de datos.

```{r eval=FALSE}
# Hagamos lo mismo con la base AA
Nueva_AA <- boxcox(AA)

# Verificar con el gráfico Q-Q Plot
qqnorm(Nueva_AA$x.t)
qqline(Nueva_AA$x.t)
  
# Verificar con el test de Shapiro-Wilk
shapiro.test(Nueva_AA$x.t)

# Grafiquemos nuevamente los boxplots AA, JN,
# Nueva_AA y Nueva_JN
par(mfrow=c(2,1))
boxplot(AA,JN)
boxplot(Nueva_AA$x.t,Nueva_JN$x.t)

# Nos adelantaremos un poco y contrastaremos las pruebas T:
#  H0: no existen diferencias significativas...
#  Ha: existen diferencias significativas...

# prueba t con corrección de welch
t.test(AA,JN) # p-value = 0.06063
t.test(Nueva_AA$x.t,JN) # p-value < 2.2e-16 obviamente
# Welch Two Sample t-test
t.test(Nueva_AA$x.t,Nueva_JN$x.t) # p-value = 1
```

# **3. Comparación de grupos (parte 1)**

En esta primera parte, aplicaremos los métodos de comparación de dos grupos, tanto a nivel paramétrico como no paramétrico.

> **Pruebas de una o dos colas**
>
> La prueba de una cola (less o greater) es apropiada si solo desea determinar si hay una diferencia entre los grupos en una dirección específica. Por lo tanto, si solo está interesado en determinar si el Grupo A muestra un promedio más alto que el Grupo B, y no estás interesado en la posibilidad de que el Grupo A muestre un promedio más bajo que el Grupo B.
>
> La principal ventaja de utilizar una prueba de una cola es que tiene más poder estadístico que una prueba de dos colas al mismo nivel de significancia (por ejemplo: 0.05). En otras palabras, es más probable que sus resultados sean significativos para una prueba de una cola si realmente hay una diferencia entre los grupos en la dirección que ha predicho.

Creemos un conjunto de datos. Crea la variable `x` conteniendo los valores de `Sepal.Length` filtrando la especie (columna `Species`) `setosa`, y la variable `y` con el filtrado para la especie `versicolor`.

```{r eval=FALSE}
# Filtra la base de datos iris
x <- iris %>% filter(iris$Species=="setosa") %>%
  pull(Sepal.Length)
y <- iris %>% filter(iris$Species=="versicolor") %>%
  pull(Sepal.Length)

# ¿Como corroboro si los grupos 
# tienen varianza diferente o similar?
# exiaten diferencias proque las cajas no se tocan
boxplot(x,y)
# leveneTest requiere tabla larga
data <- iris %>% filter(Species %in% c("setosa","versicolor"))
# prueba de homogeneidad de varianza
# resultan significativas, no son homogéneas
car::leveneTest(iris$Sepal.Length,iris$Species)
```

## **3.1 Comparación de dos grupos (Paramétrico)**

Alternativa de dos colas: la nula dice que a ambos lados no es diferente del promedio esperado.

Alternativa de less: la alterna dice que el grupo es significativamente menos que el valor esperado.

Alternativa de greater: la alterna dice que el grupo es significativamente mayor que el valor esperado.

Esto es similar para el caso de dos grupos.

```{r eval=FALSE}
# t-test de una muestra
t.test(x,mu=6) #p-value < 2.2e-16, prueba dos colas two.sides
t.test(x,mu=6,alternative = "less")
t.test(x,mu=6,alternative = "greater")

# t-test de dos muestras dependientes
t.test(x,y,paired = TRUE) # p-value = 1.242e-13

# t-test de dos muestras independientes con varianzas iguales (pooled)
# cuando son iguales las varianzas se procede a 
  t.test(x,y,var.equal = TRUE)

# t-test de dos muestras independientes con varianzas desiguales
  # con varianzas desiguales se activa la corrección de welch
t.test(x,y,var.equal = FALSE)
t.test(x,y,var.equal = FALSE, alternative = "less")
t.test(x,y,var.equal = FALSE, alternative = "greater")
```

## **3.2 Comparación de dos grupos (No Paramétrico)**

```{r eval=FALSE}
# Creamos dos variables que de antemano sabemos
# no tienen distribución normal
w <- rpois(100,lambda = 5)
z <- rpois(100,lambda = 25)

# Test de Wilcoxon de una muestra
# aquí el mu y el lambda es mediana
wilcox.test(w,mu = 4) # p-value = 0.004273
wilcox.test(w,mu = 4, alternative = "less")
wilcox.test(w,mu = 4, alternative = "greater")

# Test de Rangos con Signo de Wilcoxon 
# Para muestras pareadas (DEPENDIENTES)
# Que no tengan distribución normal
wilcox.test(w,z,paired = TRUE) p-value < 2.2e-16

# Test U de Mann-Whitney
# Para muestras no pareadas (INDEPENDIENTES)
# Que no tengan distribución normal
wilcox.test(w,z)
```

# **4. Regresiones Lineales Simples**

> **Recomendación:** Siempre que generes un modelamiento, sea del tipo que sea, es recomendable guardarlo con algún nombre asignado en el ambiente de RStudio. Esto facilita llamar el modelo en posteriores ocaciones, para ver los resultados, para comparar modelos, etc.

# Asunciones teóricas del modelo lineal

```{r eval=FALSE}
library(dslabs)
data("temp_carbon")
# las variables no deben tener NAs
temp_carbon <- na.omit(temp_carbon)
# A1: La variable respuesta debe ser continua.
str(temp_carbon) #TRUE

# A2: La relación entre X e Y debe ser lineal
# primero usando ggpubr:
library(ggpubr)
ggscatter(x="temp_anomaly" ,y="carbon_emissions", data=temp_carbon)
# con ggplot:
temp_carbon %>% 
  ggplot(aes(y=temp_anomaly ,x=carbon_emissions))+
  geom_point()+
  geom_smooth(method = "lm")

# A4: La variable respuesta no debe tener outliers en ningún nivel
# de análisis.
boxplot.stats(temp_carbon$temp_anomaly) # no outliers

```

# Ejecución del modelo lineal

```{r eval=FALSE}
# Realizar el modelo
modelo.lm <- lm(temp_anomaly~carbon_emissions, data = temp_carbon)

# A2,A4,A5,A6
# GRAFICAR LAS ASUNCIONES TEÓRICAS
par(mfrow=c(2,2))
plot(modelo.lm)
dev.off()
summary(modelo.lm)

# Gráficas de las asunciones más amigables
performance::check_model(modelo.lm)

# predicciones en base al modelo lineal
predict(modelo.lm, # -0.1491239 
        data.frame(carbon_emissions=1100))
predict(modelo.lm, # -0.2490601 
        data.frame(carbon_emissions=100))

# restando
-0.1491239--0.2490601  #0.0999362
# por cada aumento de 1000 unidades de emision de
# carbono se predice que la anomalía aumenta en 1000 unidades   

# valores de residuales calculados del modelo
resid(modelo.lm)
modelo.lm$residuals
# se puede generar una tabla con stats
tabl <- broom::augment(modelo.lm)
```

![Check model de performance](images/rstudio_cmljZRWqwN.png "Asunciones de la regresión lineal")

La figura 2 se explica sola XD

# Contrastando modelos aditivos y con interacción

```{r eval=FALSE}
# Cargamos la BD interacción
interac <- openxlsx::read.xlsx("interaccion.xlsx")
# Definiendo las categorías como factor y establecer el nivel base
# presión.Cat es caracter y podría ser factor
```

### Base de datos cargada

```{r}
openxlsx::read.xlsx("interaccion.xlsx")
```

```{r eval=FALSE}
# cohercionando a factor y reordenando los niveles
# esto ultimo es importante porque se debe definir el nivel 
# base para el intercepto
interac$Presion.Cat <- factor(interac$Presion.Cat,
                              levels = c("baja","alta"),
                              labels = c("baja"="Baja",
                                         "alta"="Alta"))

# Modelo lineal sin interacción
modelo1 <- lm(Fuerza~Temp+Presion.Cat, data = interac)

# Modelo lineal con interacción
modelo2 <- lm(Fuerza~Temp*Presion.Cat, data = interac)

# Graficando los modelos
# primero el modelo sin interacción
sjPlot::plot_model(modelo1,type = "eff",grid = T)
# con interaccióm
sjPlot::plot_model(modelo2,type="int")

# prediciendo el valor de aumento por cambio de categoría
predict(modelo1,data.frame(Temp=0,Presion.Cat="Alta"))  -
  predict(modelo1,data.frame(Temp=0,Presion.Cat="Baja"))
# se genera un aumento de 20.6427 de unidades de Fuerza

# De manera similar puede hallarse  haciendo variar la temperatura
predict(modelo1,data.frame(Temp=1,Presion.Cat="Baja"))  -
  predict(modelo1,data.frame(Temp=0,Presion.Cat="Baja"))
# 0.21919 aumento de 0.21919  unidades de fuerza por aumento en temp
```

![Gráfico de efectos del modelo1](images/rstudio_uv7WEz3wzQ.png)

La parte derecha de la figura 4 muestra un gráfico lineal, sin embargo este puede reemplazarse por uno de bigotes, que que la variable presión es categórica.

# Lidiando con outliers

```{r eval=FALSE}
# Primero boxplot
ggpubr::ggboxplot(y="Ozone",data=airquality)
# por mes
ggboxplot(y="Ozone",x="Month",data = airquality)
# Valor de la distancia intercuartil
iqr <- IQR(airquality$Ozone,na.rm = T) # 45.25

# viendo los cuantiles
quantile(airquality$Ozone,na.rm = T)
Q1 <- 18.00
Q3 <- 63.25
iqr <- Q3-Q1 # [1] 45.25

# Límite inferior
Q1-1.5*iqr #
# Límite superior
Q3+1.5*iqr # 131.125

# Verifiquemos los valores fuera de rango
sort(airquality$Ozone)
# Cuales son los valores de outliers
boxplot.stats(airquality$Ozone) # out = [1] 135 168

# trabajando con la librería  rstatix
rstatix::identify_outliers(airquality, Ozone)
```

![Outliers de Ozono usando rstatix](images/rstudio_szmIS18RJc.png)

```{r eval=FALSE}
out <- rstatix::identify_outliers(airquality, Ozone)

# limpiando los outliers
limpieza <- anti_join(airquality,out) # ya no tiene outliers (DF)

# Limpiando outliers para el caso de datos agrupados
out2 <- airquality %>% group_by(Month) %>% 
  identify_outliers(Ozone)

limpieza2 <- anti_join(airquality,out2) # # ya no tiene outliers (DF)
```

![Outliers por fecha](images/rstudio_ci61Ao0UM8.png)

Instalar librerías nuevas

```{r eval=FALSE}
install.packages("polycor")
install.packages("PMCMR")
install.packages("PMCMRplus")
install.packages("emmeans")
install.packages("pwr2")
install.packages("lattice")
install.packages("datarium")
install.packages("car")
install.packages("effsize")
install.packages("broomExtra")
install.packages("MVN")
install.packages("effectsize")
install.packages("kableExtra")
```

Librerías a usar

```{r warning=FALSE, message=FALSE}
library(tidyverse)
library(nortest)
library(rstatix)
library(polycor)
library(broom)
library(lattice)
library(PMCMR)
library(datarium)
library(effsize)
library(car)
library(ggpubr)
library(emmeans)
library(MVN)
library(effectsize)
```

# **1. ANOVA (Paramétrico)**

## **1.1 ANOVA de una vía**

Carga la base de datos `ToothGrowth` de la librería `tidyverse`.

```{r eval=FALSE}
# Carga la base de datos ToothGrowth y asignar la base
# de datos con el nombre dat
data("ToothGrowth")
datos <- ToothGrowth

# Comprobar la estructura de dat
str(datos)

# Todas las variables explicativas deben ser transformadas
# a factores ordinales o nominales
datos$dose <- factor(datos$dose, 
                     labels=c("0.5"="D0.5",
                              "1"="D1",
                              "2"="D2"))

# Cambiemos los valores de dat para entender mejor la información
datos$supp <- factor(datos$supp, labels=c("OJ"="JN", 
                                          "VC"="AA"))
str(datos)
```

### **Asunciones del ANOVA**

```{r eval=FALSE}
# A1 --------------------------------------------------------
#    La variable respuesta debe ser continua.
datos$len # si es

# A2 --------------------------------------------------------
#    Los residuales son independientes. 
# esto es teórico o específico de observación

# A3 --------------------------------------------------------
#   La variable respuesta no debe tener outliers (por grupos).

# Verificar presencia de outliers
identify_outliers(datos, len) # no hay

# por dosis si hay 1
outliers <- datos %>% group_by(dose) %>% identify_outliers(len)
ggboxplot(x="dose", y="len", data=datos)

# eliminando con anti_join
datos <- anti_join(datos, outliers)
ggboxplot(x="dose", y="len", data=datos)

# Gráfico final
# https://rpkgs.datanovia.com/ggpubr/reference/ggboxplot.html
library(ggpubr)
ggboxplot(x="dose", y="len", data=datos,
          add="jitter", fill="dose",
          palette=c("#9f60fc", "#19c6ff", "#FC4E07"))+
  stat_boxplot(geom ='errorbar', width = 0.2)+
  labs(x="Dosis", y="Longitud (cm)", fill="Dosis")+
  theme(legend.position="none")

# Hacer el modelo lineal
mod <- lm(len~dose,data = datos)
# vemos la cantidad de obs por dosis
table(datos$dose)

# A4 -------------------------------------------------------
#   Los residuales tienen distribución normal. 
datos.aug <- augment(mod)
head(datos.aug)
# con shapiro
datos.aug %>% group_by(dose) %>% 
  shapiro_test(.resid)

# usando ad test
ad.test.p <- function(x){
  test <- ad.test(x)
  test$p.value
}

# todos son normales
datos.aug %>% group_by(dose) %>% 
  summarise(pvalor=ad.test.p(.resid))

datos.aug %>% select(.resid,dose) %>% 
  group_by(Dosis=dose) %>% 
  summarise_if(is.numeric,tibble::lst(pvalor=ad.test.p))

shap <- function(x){
  stat <- shapiro_test(x)
  stat$p.value
  }
  
datos.aug %>%
  group_by(dose) %>%  
  select(.resid) %>% 
  summarise_all(tibble::lst(ADtest=ad.test.p,Shapiro=shap))
# A5 -------------------------------------------------------
#   Los residuales son homocedásticos (igual varianza) 
#      Test de Bartlett
#      H0: las varianzas de los grupos son iguales
#      Ha: las varianzas de los grupos son diferentes
bartlett.test(.resid~dose,data=datos.aug) # p-value = 0.7416
```

### **Ejecutando la prueba de ANOVA Balanceado**

Hipótesis de la prueba:

-   H0: no existen diferencias significativas entre los promedios de ninguno de los grupos evaluados.

-   Ha: existen diferencias significativas entre los promedios de al menos un par de grupos.

Con el siguiente código podrán realizar cualquier **ANOVA**, ya sea de **una vía, dos vías balanceados , tres vías balanceados, unifactorial o factorial.**

```{r eval=FALSE}
# Dos formas de hacer un anova de una vía
# Modo 1: función anova(lm())
anova(lm(len~dose,data=datos))
lm(len~dose,data=datos)|> anova()
lm(len~dose,data=datos) %>%  anova()

# Modo 2: función aov()
aov(len~dose,data=datos) %>% summary()
aov(len~dose,data=datos) |> summary()

# ambos modos dan el mismo analis de varianza
```

### **Diferentes tipos de ANOVA con las funciones aprendidas**

```{r eval=FALSE}
# ANOVA Balanceado Unifactorial de una vía
# probando la robustez con data con outlier
aov(len~dose,data = ToothGrowth) %>% summary() # p=1.23e-14
  ### Probando la robustez con el desbalanceado
  aov(len~dose,data = datos) %>% summary() # p=<2e-16

# ANOVA Balanceado Unifactorial de dos vías
# ambas vias son significativas
aov(len~dose+supp,data=ToothGrowth) %>% summary()

# ANOVA Balanceado Factorial de dos vías
# tanto las variables y la interacción son significativas
aov(len~dose*supp,data=ToothGrowth) %>% summary()
```

### **Gráficas de promedios (unifactorial y factorial)**

```{r eval=FALSE}
# Gráfico de medias por grupos unifactorial de una via
# es de rstatix

# Primero generamos una tabla  de subtitulos estadisticos
ANOVA1 <- anova_test(len~dose,data = ToothGrowth)
# extratendo las medias marginales del anova
comparaciones1 <- ToothGrowth %>% 
  emmeans_test(len~dose) %>% 
  add_xy_position(x="dose",fun = "mean_se",step.increase = 0.22)

ggline(x="dose", y="len", data=datos,add=c("boxplot","jitter"),
       plot_type = "l",color="dose")+ 
  labs(subtitle = get_test_label(ANOVA1,detailed=TRUE))+
  stat_pvalue_manual(comparaciones1,color = "darkcyan")+
  theme(legend.position = "none")+xlab("Dosis")+
  ylab("Longitud del diente")
```

![Grafico ANOVA unifactorial con pos hoc test de emmeans](images/rstudio_slyzANGa14.png)


```{r eval=FALSE}
# Gráfico de medias por grupos factorial (con interacción) de dos vias
ANOVA2 <- anova_test(len~dose*supp,data=datos)

ggline(x="dose", y="len", data=datos,add=c("mean_se","jitter"),
       plot_type = "b", color = "supp")+ 
  labs(subtitle = get_test_label(ANOVA2,detailed=TRUE))+
  theme(legend.position = "bottom")# left,right,top,bottom

# Gráfico de medias por grupos factorial SENCILLO
modelo.anova <- lm(len~dose*supp,data = datos)

library(sjPlot)
plot_model(modelo.anova,type="int")
```

![Gráfico de interacción con ggline (mejor que sjplot)](images/rstudio_PtBW5aCtoF.png)

## **Test Post Hoc**

### **Prueba de Tukey**

```{r eval=FALSE}
# Primero guarda el modelo 
mod_anova <- aov(len~dose,data = datos)

# Luego, usa la función para la prueba de Tukey
# de stats
tuk <- TukeyHSD(mod_anova) #d2<d1<d0.5 
tuk2 <- as.data.frame(tuk$dose)
plot(TukeyHSD(mod_anova))
# de rstatiz
tukey_hsd(mod_anova)
# Exportar resultados a excel
# con pipes rstatix
mod_anova %>% tukey_hsd() %>% 
  openxlsx::write.xlsx("Tukey.xlsx")
# simple con pipes
openxlsx::write.xlsx(tuk2,"tuk.xlsx",overwrite = T)

# ¿Dónde se guardó?
"C:/Users/Bryan/Documents/DataScience/C1-Semana1/RepasoR2/
RepasoR2"

# Tukey de ANOVA Balanceado Unifactorial de una vía
aov(len~dose,data = datos) %>% TukeyHSD()

# Tukey de ANOVA Balanceado Unifactorial de dos vías
aov(len~dose+supp,data=datos) %>% tukey_hsd()
# parte en 2 tablas, es mejor rstatix a menos que se
# requiera más orden
aov(len~dose+supp,data=datos) %>% TukeyHSD()
# Tukey de ANOVA Balanceado Factorial de dos vías
aov(len~dose*supp,data=datos) %>% tukey_hsd()
aov(len~dose*supp,data=datos) %>% TukeyHSD()

# Graficamos Tukey de una via
ANOVAt <- anova_test(len~dose,data=ToothGrowth)
comparacion2 <- ToothGrowth %>% 
    tukey_hsd(len~dose) %>% 
    add_xy_position(x="dose",fun = "mean_se",step.increase = 0.22)

ggline(x="dose", y="len", data=ToothGrowth,add=c("boxplot",
                                           "jitter"),
       plot_type = "l",color="dose")+ 
  labs(subtitle = get_test_label(ANOVAt,detailed=TRUE))+
  stat_pvalue_manual(comparacion2,color = "darkcyan")+
  theme(legend.position = "none")+xlab("Dosis")+
  ylab("Longitud del diente")
```

### **Prueba de Scheffe**

```{r eval=FALSE}
library(DescTools)
mod_anova <- aov(len~dose,data = datos)
sheffe <-ScheffeTest(mod_anova)
sheffe$dose %>% as_tibble() %>% 
  openxlsx::write.xlsx("schefe.xlsx")
```

### **Prueba de Duncan**

```{r eval=FALSE}
library(PMCMRplus)
du <- duncanTest(mod_anova)
du$p.value
```

# **2. Anova (No Paramétrico):** Test de Kruskal-Wallis

Carga la base de datos `ChickWeight`. Estas son mediciones de pesos (columna `weight`) de pollitos que fueron sometidos a 4 dietas diferentes (columna `Diet`).

```{r eval=FALSE}
data("ChickWeight")
pesos <- ChickWeight
head(ChickWeight)
# Normalidad de los residuales
mod_pesos <- lm(weight~Diet,data=pesos)
mod_pesos2 <- augment(mod_pesos)
mod_pesos2 %>% 
  group_by(Diet) %>% 
  summarise(residuales=ad.test.p(.resid))
# no hay normalidad
```

### **Asunciones de Kruskal-Wallis**

```{r eval=FALSE}
# A4 -------------------------------------------------------
#   Observar la distribución de las funciones de densidad de la
#   variable respuesta para cada grupo del factor  los datos
# deben tener una sola distribución
library(lattice)
densityplot(~weight,groups=Diet,data = pesos)
```

![Datos sesgados a la izquierda](images/rstudio_ffRRuJNdag.png)

### Ejecutar la prueba de Kruskal-Wallis

```{r eval=FALSE}
# Realizar el test de Kruskal Wallis
kruskal.test(weight~Diet,data=pesos) # p-value = 2.012e-05
# Realizar el Post hoc, test de dunn comparaciones pareadas
# con corrección de holm y bonferroni, metodos de ajustes
# por defecto ofrece holm
dunn_test(weight~Diet,data = pesos
          ,p.adjust.method = "holm")
# con corrección de bonferroni
dunn_test(weight~Diet,data = pesos
          ,p.adjust.method = "bonferroni")
```

# **3. ANOVAS No Balanceados**

## 3.1 Anova Tipo II y Tipo III

Antes de realizar estos ANOVAs es necesario revisar los supuestos teóricos previamente descritos. Sucede que estos tipos de anova tienen una diferente manera de calcular sus sumas de cuadrados.

> -   Tipo 2: Recomendable cuando **no existe** interacción significativa entre los factores del modelo. Esto se traduce en que el tipo 2 es más robusto cuando no hay interacción significativa.
>
> -   Tipo 3: Si la interacción **es significativa**, entonces es mejor el tipo 3.

```{r eval=FALSE}
# Carga la base de datos heartattack.xlsx
ac <- openxlsx::read.xlsx("heartattack.xlsx")
ac <- openxlsx::read.xlsx(file.choose())
str(ac)
# las conversiones a factor no son tan necesarias en funciones
# anova y de modelo lineal, pero es una buena práctica
ac2 <- ac %>% mutate_if(is.character,as.factor)
# Verifiquemos si los factores son desbalanceadas o no
# resultan si ser desbalanceados
table(ac2$gender)
table(ac2$drug)
table(ac2$risk)

# Revisar si hay interacción significativa
anovax <- aov(cholesterol~gender*drug,data=ac2)
summary(anovax)

# Crea el modelo lineal final 
modFinal <- aov(cholesterol~gender+drug,data = ac2)

# Ejecuta el ANOVA Final, 
# decidiendo el tipo que corresponda (II o III)
#escogemos 2 porque es el adecuado sin interacciones
# ñsignificativas
library(car)
Anova(modFinal,type = "II")

# PostHoc de ANOVAS no Balanceados (interpretar los significativos)
aov(cholesterol~gender+drug,data = ac2) %>% 
  TukeyHSD()

library(sjPlot)
plot_model(modFinal,type="eff")

ggline(x="gender", y="cholesterol", data=ac2,
       add=c("boxplot","jitter"),
       plot_type = "l",color="gender")

ggline(x="drug", y="cholesterol", data=ac2,
       add=c("boxplot","jitter"),
       plot_type = "p",color="drug")

ac2 %>% group_by(drug) %>% select(cholesterol) %>% 
  summarise(prom=median(cholesterol))
```

# **4. ANCOVA**

Carga el excel `contaminacion.xlsx`. Esta base de datos nos servirá para identificar cuál es el protocolo que nos genera una menor cantidad de contaminación, teniendo en cuenta la covariable tiempo de actividad diario.

### **Asunciones teóricas del ANCOVA**

```{r eval=FALSE}
# Carga el excel contaminacion.xlsx
contaminacion <- openxlsx::read.xlsx("contaminacion.xlsx")
contaminacion <- openxlsx::read.xlsx(file.choose())
contaminacion <- contaminacion %>% 
  mutate_if(is.character,as.factor)

# A2 ---------------------------------------------------------------
#   Verificar presencia de outliers.
#   no hay outliers
ggboxplot(x="trat",y="contam",data = contaminacion)
contaminacion %>% group_by(trat) %>% 
  identify_outliers(contam)

# A3 ---------------------------------------------------------------
#   Linealidad entre la covariable y la variable respuesta 
# (Y).
ggscatter(x="tiempo",y="contam",color = "trat",
          data = contaminacion,size = 3,add = "reg.line")+
  facet_wrap(~trat)

# A4 ---------------------------------------------------------------
#   Independencia de la covariable y la variable explicativa (X).
#     Verificar que al enfrentar en un ANOVA la covariable vs
#     la variable respuesta X, el pvalor sea no significativo (p>0.05)
# vemos que hay independencia 
aov(tiempo~trat,data = contaminacion) %>% summary()

# A5 ---------------------------------------------------------------
#   Homogeneidad las pendientes de la regresión por grupo
#     No debe existir interacción entre covariable y var. explicativa (X).
ggscatter(x="tiempo",y="contam",color = "trat",
          data = contaminacion,size = 3,add = "reg.line")

  # A6 ---------------------------------------------------------------
#   Los residuales del modelo ANCOVA tienen distribución normal. Algo curioso es que los residuales de un lm y 
# un anova tienen un p de normalidad identico
ancova_mod <- aov(contam~tiempo+trat,data=contaminacion)
ad.test(resid(ancova_mod)) # p-value = 0.5076

# A7 ---------------------------------------------------------------
#   Homocedasticidad (homogeneidad de las varianzas) de los residuales.
# resulta que si es homogeneo
ancova_mod <- lm(contam~tiempo+trat,data=contaminacion)
au.lm.test <- augment(ancova_mod)
bartlett.test(.resid~trat,data=au.lm.test) # p-value = 0.1751
```

### Ejecutar el ANCOVA

```{r eval=FALSE}
# Si todo está conforme, ejecuta el ANCOVA
anova(ancova_mod)
```

### Post Hoc para el ANCOVA

Post Hoc para el ANCOVA se realiza calculando los promedios marginales, que es la forman de nombrar los promedios de cada grupo.

```{r eval=FALSE}
# Calculamos los promedios marginales estimados usando la funcion emmeans()
library(emmeans)
em <- emmeans(ancova_mod,specs = "trat")
em

# Visualizamos los contrastes pareados para identificar 
# la significancia de las comparaciones de promedios de grupo.
# Estimate es el valor de la diferencia entre los tratamientos en cuestión.
pairs(em)
```

# **5. Análisis de Correlación**

Carga el excel `correlacion.xlsx` y asígnale el nombre `corr`.

```{r eval=FALSE}
# Carga el excel correlacion.xlsx y asígnale el nombre corr
corr <- openxlsx::read.xlsx("correlacion.xlsx")
corr <- openxlsx::read.xlsx(file.choose())
```

## 5.1 Pearson

> -   A1: Ambas variables deben haber sido tomadas como pares relacionados.
>
> -   A2: Ambos conjuntos enfrentados deben ser continuos.
>
> -   A3: Debe haber una relación lineal entre las variables.
>
> -   A4: No deben existir outliers en los conjuntos de datos
>
> -   A5: Ambos conjuntos de datos deben seguir la distribución normal bivariada.

Probemos con dos ejemplos de correlaciones: `biomass~height` y `height~DBH`.

```{r eval=FALSE}
view(corr)
# A3 ------------------------------------------------------------------- 
#   Verificar relación lineal de las variables
ggscatter(x="biomass",y="height",data=corr)

# A4 ------------------------------------------------------------------- 
#   Verificar presencia de outliers
# no hay outliers
ggboxplot(y=c("biomass","height"),data=corr)

# A5 ------------------------------------------------------------------- 
#   Verificar distribución normal univariada
library(nortest)
ad.test(corr$biomass) # normal p-value = 0.5125
ad.test(corr$height) # normal p-value = 0.9539

#   Verificar distribución normal bivariada: Test de Mardia
library(MVN)
datos_corr<- corr %>% select(biomass,height) 
datos.bnd <- mvn(datos_corr,mvnTest = "royston",
                 univariateTest = "AD")
datos.bnd <- mvn(datos_corr, mvnTest = "mardia",
                 univariateTest="SW") # si hay normalidad multivariante

#  ---------------------------------------------------------------------

# EJECUTAR EL TEST
# Realizar la correlación
cor.test(corr$biomass,corr$height,method = "pearson")
cor.test(corr$height,corr$biomass,method = "pearson")
# p-value = 5.611e-12; corr = 0.6673333
```

## 5.2 Spearman/Kendall

Usada cuando no se cumple con las asunciones de pearson. Usemos el ejemplo que no se pudo ejecutar en la sección de la correlación de Pearson.

```{r eval=FALSE}
# A2 ------------------------------------------------------------------- 
#   Verificar que no hay distribución normal bivariada: Test de Mardia
datos.corr2 <- corr %>% select(DBH,height)
datos.bnd2 <- mvn(datos.corr2,mvnTest="royston") #no hay norm multi

# A3 ------------------------------------------------------------------- 
#   Verificar relación monotónica (i.e., en un solo sentido)
ggscatter(x="DBH",y="height",data = datos.corr2)
#  ---------------------------------------------------------------------

# EJECUTAR EL TEST
# Correlación de Spearman
cor.test(corr$DBH,corr$height,method = "spearman")
# rho=0.8804918

# Correlación de Kendall
cor.test(corr$DBH,corr$height,method = "kendall")
# tau= 0.7040471
```

## 5.3 Punto Biserial

Esta es una extensión de la correlación de Pearson, por ello se usa la misma función revisada en la sección 6.1. Contrastaremos la variable `LDL` en mg/dL vs el estatus dicotómico de `obesidad`.

```{r eval=FALSE}
# A2 ------------------------------------------------------------------- 
#   Verifica la presencia de outliers en la variable cuantitativa 
view(corr)
corr %>% identify_outliers(LDL) # <0 rows> (or 0-length row.names)
#  ---------------------------------------------------------------------

# Visualiza la relación
ggscatter(x="LDL",y="obesidad",data = corr)

# Realiza la correlación Punto Biserial (LDL vs obesidad)
cor.test(corr$LDL,corr$obesidad,method = "pearson")#cor=0.6448317
```

**La correlación es alta positiva (rho=0.6448317):**

El pertenecer a la categoría 0 (no obeso) de la variable categórica guarda una alta correlación con bajos niveles de LDL (colesterol malo). Mientras que los que pertenecen a la categoría 1 (obeso) guardan una alta correlación con niveles altos de LDL (rho=0.65, p-value \<0.001).

## 5.4 Biserial

Contrastaremos la variable LDL en mg/dL vs la versión dicotomizada de la variable presión arterial. Aquí ya no testeamos la presencia de outliers en la variable cuantitativa porque ya lo hicimos en la sección anterior. No obstante, recuerda siempre revisar este supuesto teórico.

```{r eval=FALSE}
# A2 ------------------------------------------------------------------- 
#   Verifica la presencia de outliers en la variable cuantitativa 
corr %>% identify_outliers(LDL) #<0 rows> (or 0-length row.names)

#  ---------------------------------------------------------------------

# Realiza la correlación Biserial
library(polycor)
polyserial(corr$LDL,corr$presDico) # -0.9141895

# Visualiza la relación
ggscatter(x="LDL",y="presDicoDummy",data = corr, add = "reg.line")
# no se debe poner una linea de regresión, solo para entender!
```

![Gráfico Biserial de presión arterial dicotomizada vs colesterol malo](images/rstudio_3Vh1BAv6cR.png)

# **6. Poder estadístico**

## 6.1 Tamaño del efecto: g

Aquí revisaremos el cálculo del tamaño del efecto como `d de Cohens` o `g de Hedges` que nos sirven cuando comparamos dos grupos (Pruebas de T). El tamaño del efecto para ANOVAS, es decir, para cuando lidiamos con más de dos grupos, los calcularemos en las siguientes secciones.

> -   Para la prueba t se usa **d** de cohen, corregido como **g** de hedges.
>
> -   Para Wilcoxon, U de Mann Whithey **delta** de Cliff
>
> -   Para correlaciones es el mismo **valor de correlación**
>
> -   Para regresiones es el **R^2^**
>
> -   Para Anova es el Eta cuadrado **n^2^**
>
> -   La opción menos sesgadas para N pequeños es el Omega cuadrado **w^2^**

```{r eval=FALSE}
# Cargar la base de datos mice2 de la librería datarium
data("mice2")

# Transformemos hacia tabla larga para poder procesar mejor 
# con la función de tamaño de efecto para comparaciones de dos grupos
mice3 <- mice2 %>% 
  gather(key="categoria", value="pesos", 2:3)

# Calcula el tamaño del efecto para comparaciones de dos grupos
library(effsize)
cohen.d(mice3$pesos,mice3$categoria)
# d estimate: 7.805217 (large), con 1 era suficiente xd
# con valores superiores a 2 con un pequeño n muestral se halla
# un 100% de poder de la muestra
```

## 6.2 Cálculo del Poder: Prueba de T

```{r eval=FALSE}
# Obtén el valor de g (tamaño del efecto con correción de Hedges)
cohen.d(mice3$pesos,mice3$categoria, hedges.correction = TRUE)
# g estimate: 7.475419 (large)

# Número de observaciones por grupo (n)
table(mice3$categoria)
# Poder estadístico del análisis
# Calcula el poder de la Prueba T con mismo N por grupos
library(pwr)
pwr.t.test(n=10,d=7.475419,sig.level=0.05)
# power = 1

# Calcula el poder de la Prueba T con diferente N por grupos
# colocan el n de cada grupo en n1 y n2.
pwr.2p2n.test(n1=10,n2=10,h=7.475419,sig.level=0.05)
```

### **Gráfico del Poder para identificar el N muestral óptimo**

Veamos cuánto N muestral necesitaríamos para obtener un tamaño de efecto determinado para una muestra que en el testeo preliminar nos arrojó un tamaño de efecto de 0.3.

```{r eval=FALSE}
poder <- pwr.t.test(d=0.3,power = 0.75,sig.level = 0.05)
plot(poder) # muy util plotear para obtener los poderes
# en la curva
```

## 6.3 Cálculo del Poder: Anova de una vía balanceado

```{r eval=FALSE}
# Definir el ANOVA a trabajar
a1v <- anova(lm(len~dose, data=ToothGrowth))

# Calcular el tamaño del efecto del ANOVA de una vía
# Identifica el número de muestras para decidir entre 
# eta cuadrado u omega cuadrado
effectsize::eta_squared(a1v) # etasqr=0.64
effectsize::omega_squared(a1v) # omega2=0.63, para db smalls

# Verificar el número de casos en la cada variable de agrupamiento
table(ToothGrowth$dose)

# Poder estadístico del análisis
library(pwr)
pwr.anova.test(n=20,k=3,sig.level = 0.05,f=0.63)
# power = 0.9928436
pwr.anova.test(power=0.75,k=3,sig.level = 0.05,f=0.73)
# n = 6.480334, casi 7 muestras

# Ahora, sabiendo el poder que queremos, identifiquemos el 
# n muestral necesario para obtenerlo.
poder.aov1 <- pwr.anova.test(power=0.75,k=3,sig.level = 0.05,f=0.73)

# Gráfico del Poder para identificar el N muestral óptimo
plot(poder.aov1)
```

## 6.4 Cálculo del Poder: Anova de dos vías balanceado

```{r eval=FALSE}
# Definir el ANOVA a trabajar
a2v <- anova(lm(len~dose+supp, data=ToothGrowth))

# Calcular el tamaño del efecto del ANOVA de dos vías
# Identifica el número de muestras para decidir entre 
# eta cuadrado u omega cuadrado
effectsize::omega_squared(a2v) # dose=0.67,supp=0.15

# Verificar el número de casos por cada variable de agrupamiento


# Poder estadístico del análisis
library(pwr2)
pwr.2way(alpha = 0.05 , # nivel de significancia
         size.A = 20,    # N del primer factor para cada grupo
         size.B = 30,    # N del segundo factor para cada grupo
         f.A = 0.67   ,     # Tamaño del efecto del primer factor
         f.B =0.15,     # Tamaño del efecto del segundo factor
         a = 3     ,         # Número de grupos del primer factor
         b = 2 )         # Número de grupos del segundo factor
# power.A = 0.9999995, power.B = 0.5165924,power = 0.5165924
# no tiene gráfico XD
```

Para otras pruebas de las que no se tengan una función para obtener estos análisis del poder estadístico, pueden usar simulaciones de Monte Carlo, con el enfoque de un análisis Bayesiano: Bayesian Power Analysis. Uno de los mejores ejemplos de su uso pueden encontrarlo a detalle en el siguiente RPubs:

[RPubs - Generic Bayesian power analysis](https://rpubs.com/kachergis/Generic_Bayesian_power)





